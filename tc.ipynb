{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create all rules\n",
    "def exeva(i,a,s,num):\n",
    "    if i == num:\n",
    "        return a, s\n",
    "    \n",
    "    tempa = []\n",
    "    temps = []\n",
    "    \n",
    "    temp = exeva(i+1,a+[1]+[0]+[0],s+'1',num)\n",
    "    if i == num-1:\n",
    "        tempa.append(temp[0])\n",
    "        temps.append(temp[1])\n",
    "    else:\n",
    "        tempa.extend(temp[0])\n",
    "        temps.extend(temp[1])\n",
    "    \n",
    "    temp = exeva(i+1,a+[0]+[1]+[0],s+'2',num)\n",
    "    if i == num-1:\n",
    "        tempa.append(temp[0])\n",
    "        temps.append(temp[1])\n",
    "    else:\n",
    "        tempa.extend(temp[0])\n",
    "        temps.extend(temp[1])\n",
    "    \n",
    "    temp = exeva(i+1,a+[0]+[0]+[1],s+'3',num)\n",
    "    if i == num-1:\n",
    "        tempa.append(temp[0])\n",
    "        temps.append(temp[1])\n",
    "    else:\n",
    "        tempa.extend(temp[0])\n",
    "        temps.extend(temp[1])\n",
    "    \n",
    "    return tempa, temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(filename, dimension):\n",
    "    #read data\n",
    "    xdata = []\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "    tp = 0\n",
    "    tdata = []\n",
    "    for line in lines:\n",
    "        tdata.append(float(line))\n",
    "        tp = tp + 1\n",
    "        if tp==dimension:\n",
    "            tp = 0\n",
    "            xdata.append(tdata)\n",
    "            tdata = []\n",
    "    return xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    #placeholder\n",
    "    x = tf.placeholder(tf.float32, [None, D_in])\n",
    "    y = tf.placeholder(tf.float32, [None, D_out])\n",
    "    w = {\n",
    "        'h1': tf.Variable(tf.random_normal([D_in, D_h1])),\n",
    "        'h2': tf.Variable(tf.random_normal([D_h1, D_h2])),\n",
    "        'h3': tf.Variable(tf.random_normal([D_h2, D_h3])),\n",
    "        'h4': tf.Variable(tf.random_normal([D_h3, D_h4])),\n",
    "        #'h5': tf.Variable(tf.random_normal([D_h4, D_h5])),\n",
    "        'out': tf.Variable(tf.random_normal([D_h4, D_out]))\n",
    "    }\n",
    "    b = {\n",
    "        'h1': tf.Variable(tf.random_normal([D_h1])),\n",
    "        'h2': tf.Variable(tf.random_normal([D_h2])),\n",
    "        'h3': tf.Variable(tf.random_normal([D_h3])),\n",
    "        'h4': tf.Variable(tf.random_normal([D_h4])),\n",
    "        #'h5': tf.Variable(tf.random_normal([D_h5])),\n",
    "        'out': tf.Variable(tf.random_normal([D_out]))\n",
    "    }\n",
    "\n",
    "    #activation functions #tf.layers.batch_normalization()\n",
    "    def multilayer_perceptron(x):\n",
    "        h1_layer = tf.sigmoid(tf.add(tf.matmul(x, w['h1']), b['h1']))\n",
    "        h2_layer = tf.sigmoid(tf.add(tf.matmul(h1_layer, w['h2']), b['h2']))\n",
    "        h3_layer = tf.sigmoid(tf.add(tf.matmul(h2_layer, w['h3']), b['h3']))\n",
    "        h4_layer = tf.sigmoid(tf.add(tf.matmul(h3_layer, w['h4']), b['h4']))\n",
    "        #h5_layer = tf.sigmoid(tf.add(tf.matmul(h4_layer, w['h5']), b['h5']))\n",
    "        out_layer = tf.sigmoid(tf.add(tf.matmul(h4_layer, w['out']), b['out']))\n",
    "        return out_layer\n",
    "    pred = multilayer_perceptron(x)\n",
    "\n",
    "    #loss function\n",
    "    #cost = tf.reduce_sum(tf.where(tf.equal(0.0, y), abs(y-pred),\n",
    "    #                     tf.where(tf.equal(1.0, y), abs(y-pred),\n",
    "    #                     tf.where(tf.equal(-1.0, y), y-y,\n",
    "    #                     (y-pred)*(y-pred)))))\n",
    "    cost = tf.reduce_sum((y-pred)*(y-pred))\n",
    "\n",
    "    #optimizer and others\n",
    "    optimizer = tf.train.AdamOptimizer(0.002).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    variables_dict = {\n",
    "        'b1': b['h1'],\n",
    "        'b2': b['h2'],\n",
    "        'b3': b['h3'],\n",
    "        'b4': b['h4'],\n",
    "        #'b5': b['h5'],\n",
    "        'bout': b['out'],\n",
    "        'w1': w['h1'],\n",
    "        'w2': w['h2'],\n",
    "        'w3': w['h3'],\n",
    "        'w4': w['h4'],\n",
    "        #'w5': w['h5'],\n",
    "        'wout': w['out']\n",
    "    }\n",
    "    saver = tf.train.Saver(variables_dict)\n",
    "    \n",
    "    tf.summary.scalar('loss',cost)\n",
    "    tf.summary.histogram('b1', b['h1'])\n",
    "    tf.summary.histogram('b2', b['h2'])\n",
    "    tf.summary.histogram('b3', b['h3'])\n",
    "    tf.summary.histogram('b4', b['h4'])\n",
    "    tf.summary.histogram('bout', b['out'])\n",
    "    tf.summary.histogram('w1', w['h1'])\n",
    "    tf.summary.histogram('w2', w['h2'])\n",
    "    tf.summary.histogram('w3', w['h3'])\n",
    "    tf.summary.histogram('w4', w['h4'])\n",
    "    tf.summary.histogram('wout', w['out'])\n",
    "    \n",
    "    return x, y, pred, cost, optimizer, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(write):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, \"./model.ckpt\")\n",
    "        result = pred.eval(feed_dict = {x: edata})\n",
    "    if write:\n",
    "        with open('nnr.txt', 'w') as f:\n",
    "            for result1 in result:\n",
    "                for result2 in result1:\n",
    "                    f.write(str(result2) + '\\n')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        merged_all = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num/batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_xs = xdata[i*batch_size:(i+1)*batch_size]\n",
    "                batch_ys = ydata[i*batch_size:(i+1)*batch_size]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys})\n",
    "                avg_cost += c / total_batch\n",
    "            if (epoch+1) % display_step == 0 or epoch == 0:\n",
    "                print(\"Training Loop \"+str(loopcount), \"Epoch:\", '%04d' % (epoch+1), \" cost=\", \"{:.9f}\".format(avg_cost))\n",
    "                #merged = sess.run(merged_all, feed_dict={x: batch_xs, y: batch_ys})\n",
    "                #writer.add_summary(merged, epoch)\n",
    "            if (epoch+1) % save_step == 0:\n",
    "                saver.save(sess, \"./model.ckpt\")\n",
    "                print(\"save model done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CV(bearcount):\n",
    "    minscore = 1.0\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        merged_all = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num/batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_xs = xdata[i*batch_size:(i+1)*batch_size]\n",
    "                batch_ys = ydata[i*batch_size:(i+1)*batch_size]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys})\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            evadata = pred.eval(feed_dict = {x: edata})\n",
    "            score = 0\n",
    "            count = count + 1\n",
    "            for j in range(len(evadata)):\n",
    "                score = score + abs(evadata[j]-rdata[j])/len(evadata)\n",
    "            if score<minscore:\n",
    "                minscore = score\n",
    "                count = 0\n",
    "            if count>bearcount:\n",
    "                saver.save(sess, \"./model.ckpt\")\n",
    "                print(\"save model done.\")\n",
    "                print(\"Training Loop \"+str(loopcount), \"Epoch:\", '%04d' % (epoch+1), \" cost=\", \"{:.9f}\".format(avg_cost))\n",
    "                print('Training Loop '+str(loopcount)+' Epoch: '+str((epoch+1))+' no further improvement in the testing set, stop.')\n",
    "                print('current score '+str(score)+' minscore '+str(minscore))\n",
    "                break\n",
    "            \n",
    "            if (epoch+1) % display_step == 0 or epoch == 0:\n",
    "                print(\"Training Loop \"+str(loopcount), \"Epoch:\", '%04d' % (epoch+1), \" cost=\", \"{:.9f}\".format(avg_cost))\n",
    "                #merged = sess.run(merged_all, feed_dict={x: batch_xs, y: batch_ys})\n",
    "                #writer.add_summary(merged, epoch)\n",
    "            if (epoch+1) % save_step == 0:\n",
    "                saver.save(sess, \"./model.ckpt\")\n",
    "                print(\"save model done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find():\n",
    "    #generate all rules\n",
    "    exdata, exdatastr = exeva(0,[],'2',13)\n",
    "    \n",
    "    #predict output\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"./model.ckpt\")\n",
    "        result = pred.eval(feed_dict = {x: exdata})\n",
    "\n",
    "    #sort results\n",
    "    sortedresult = []\n",
    "    for i in range(len(result)):\n",
    "        sortedresult.append((exdatastr[i],result[i]))\n",
    "    dtype = [('Rule', 'S14'),('Value', float)]\n",
    "    sortedresult = np.array(sortedresult, dtype=dtype)\n",
    "    sortedresult = np.sort(sortedresult, order='Value')\n",
    "    \n",
    "    #write new rules, top 100 minimum, top 100 maximum, equally-separated 100\n",
    "    with open('newrules.txt', 'w') as f:\n",
    "        for i in range(len(sortedresult)):\n",
    "            if (i<100) or (i>len(sortedresult)-1-100) or (i%((len(sortedresult)-200)//100)==0):\n",
    "                f.write(str(sortedresult[i][0])[2:-1] + '\\n')\n",
    "\n",
    "    #write new volumes\n",
    "    with open('newvolumes.txt', 'w') as f:\n",
    "        for i in range(len(sortedresult)):\n",
    "            if (i<100) or (i>len(sortedresult)-1-100) or (i%((len(sortedresult)-200)//100)==0):\n",
    "                f.write(str(sortedresult[i][1]) + '\\n')\n",
    "                \n",
    "    print('find doen!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    xx = []\n",
    "    yy = []\n",
    "    pp = [True for i in range(len(xdata))]\n",
    "    for i in range(len(xdata)-1):\n",
    "        for j in range(i+1,len(xdata)):\n",
    "            pd = True\n",
    "            for k in range(D_in):\n",
    "                if xdata[i][k] != xdata[j][k]:\n",
    "                    pd = False\n",
    "                    break\n",
    "            if pd == True:\n",
    "                pp[i] = False\n",
    "                break\n",
    "    for i in range(len(xdata)):\n",
    "        if pp[i] == True:\n",
    "            xx.append(xdata[i])\n",
    "            yy.append(ydata[i])\n",
    "    print('clean doen! '+str(len(xx))+' instances from '+str(len(xdata))+' instances left.')\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffledata():\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    xdatatotal = xdata\n",
    "    ydatatotal = ydata\n",
    "    numtotal = len(xdatatotal)\n",
    "    datatotal = []\n",
    "    for i in range(numtotal):\n",
    "        xdatatotal[i].extend(ydatatotal[i])\n",
    "        datatotal.append(xdatatotal[i])\n",
    "    random.shuffle(datatotal)\n",
    "    \n",
    "    xx = [[] for i in range(len(datatotal))]\n",
    "    yy = [[] for i in range(len(datatotal))]\n",
    "    for i in range(len(datatotal)):\n",
    "        for j in range(D_in):\n",
    "            xx[i].append(datatotal[i][j])\n",
    "        for j in range(D_in,D_in+D_out):\n",
    "            yy[i].append(datatotal[i][j])\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign(fold):\n",
    "    xx = []\n",
    "    yy = []\n",
    "    ee = []\n",
    "    rr = []\n",
    "    for j in range(len(xdata)):\n",
    "        if j<=int(len(xdata)/fold):\n",
    "            ee.append(xdata[j])\n",
    "            rr.append(ydata[j])\n",
    "        else:\n",
    "            xx.append(xdata[j])\n",
    "            yy.append(ydata[j])\n",
    "    return xx, yy, ee, rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loop 0 Epoch: 0001  cost= 0.653845139\n",
      "Training Loop 0 Epoch: 0100  cost= 0.000340266\n",
      "Training Loop 0 Epoch: 0200  cost= 0.000263941\n",
      "Training Loop 0 Epoch: 0300  cost= 0.000991438\n",
      "Training Loop 0 Epoch: 0400  cost= 0.000398808\n",
      "Training Loop 0 Epoch: 0500  cost= 0.000316970\n",
      "Training Loop 0 Epoch: 0600  cost= 0.002351369\n",
      "Training Loop 0 Epoch: 0700  cost= 0.000185682\n",
      "Training Loop 0 Epoch: 0800  cost= 0.000260276\n",
      "Training Loop 0 Epoch: 0900  cost= 0.000432813\n",
      "Training Loop 0 Epoch: 1000  cost= 0.001182136\n",
      "Training Loop 0 Epoch: 1100  cost= 0.004502316\n",
      "Training Loop 0 Epoch: 1200  cost= 0.004144495\n",
      "Training Loop 0 Epoch: 1300  cost= 0.002929034\n",
      "Training Loop 0 Epoch: 1400  cost= 0.003489850\n",
      "Training Loop 0 Epoch: 1500  cost= 0.007252704\n",
      "Training Loop 0 Epoch: 1600  cost= 0.001719130\n",
      "Training Loop 0 Epoch: 1700  cost= 0.006686851\n",
      "Training Loop 0 Epoch: 1800  cost= 0.002781973\n",
      "Training Loop 0 Epoch: 1900  cost= 0.004034559\n",
      "Training Loop 0 Epoch: 2000  cost= 0.003368126\n",
      "Training Loop 0 Epoch: 2100  cost= 0.003224465\n",
      "Training Loop 0 Epoch: 2200  cost= 0.004449954\n",
      "Training Loop 0 Epoch: 2300  cost= 0.002837968\n",
      "Training Loop 0 Epoch: 2400  cost= 0.004198591\n",
      "Training Loop 0 Epoch: 2500  cost= 0.003004670\n",
      "Training Loop 0 Epoch: 2600  cost= 0.003374135\n",
      "Training Loop 0 Epoch: 2700  cost= 0.003477421\n",
      "Training Loop 0 Epoch: 2800  cost= 0.003386196\n",
      "Training Loop 0 Epoch: 2900  cost= 0.003734344\n",
      "Training Loop 0 Epoch: 3000  cost= 0.002511794\n",
      "Training Loop 0 Epoch: 3100  cost= 0.002982261\n",
      "Training Loop 0 Epoch: 3200  cost= 0.003196074\n",
      "Training Loop 0 Epoch: 3300  cost= 0.003472734\n",
      "Training Loop 0 Epoch: 3400  cost= 0.003049541\n",
      "Training Loop 0 Epoch: 3500  cost= 0.002457730\n",
      "Training Loop 0 Epoch: 3600  cost= 0.003628720\n",
      "Training Loop 0 Epoch: 3700  cost= 0.002556005\n",
      "Training Loop 0 Epoch: 3800  cost= 0.003878105\n",
      "Training Loop 0 Epoch: 3900  cost= 0.003949836\n",
      "Training Loop 0 Epoch: 4000  cost= 0.004355340\n",
      "Training Loop 0 Epoch: 4100  cost= 0.003649561\n",
      "Training Loop 0 Epoch: 4200  cost= 0.002668539\n",
      "Training Loop 0 Epoch: 4300  cost= 0.004206497\n",
      "Training Loop 0 Epoch: 4400  cost= 0.005184077\n",
      "Training Loop 0 Epoch: 4500  cost= 0.002838055\n",
      "Training Loop 0 Epoch: 4600  cost= 0.001712300\n",
      "Training Loop 0 Epoch: 4700  cost= 0.001338701\n",
      "Training Loop 0 Epoch: 4800  cost= 0.001022116\n",
      "Training Loop 0 Epoch: 4900  cost= 0.001613737\n",
      "Training Loop 0 Epoch: 5000  cost= 0.001822283\n",
      "save model done.\n",
      "Training Loop 0 Epoch: 5100  cost= 0.005253916\n",
      "Training Loop 0 Epoch: 5200  cost= 0.004506008\n",
      "Training Loop 0 Epoch: 5300  cost= 0.002161326\n",
      "Training Loop 0 Epoch: 5400  cost= 0.001453276\n",
      "Training Loop 0 Epoch: 5500  cost= 0.001671970\n",
      "Training Loop 0 Epoch: 5600  cost= 0.001721781\n",
      "Training Loop 0 Epoch: 5700  cost= 0.002227692\n",
      "Training Loop 0 Epoch: 5800  cost= 0.003470469\n",
      "Training Loop 0 Epoch: 5900  cost= 0.003243353\n",
      "Training Loop 0 Epoch: 6000  cost= 0.003125677\n",
      "Training Loop 0 Epoch: 6100  cost= 0.004241550\n",
      "Training Loop 0 Epoch: 6200  cost= 0.002319949\n",
      "Training Loop 0 Epoch: 6300  cost= 0.001791390\n",
      "Training Loop 0 Epoch: 6400  cost= 0.001231338\n",
      "Training Loop 0 Epoch: 6500  cost= 0.001376606\n",
      "Training Loop 0 Epoch: 6600  cost= 0.002663351\n",
      "Training Loop 0 Epoch: 6700  cost= 0.005999474\n",
      "Training Loop 0 Epoch: 6800  cost= 0.003596256\n",
      "Training Loop 0 Epoch: 6900  cost= 0.002471676\n",
      "Training Loop 0 Epoch: 7000  cost= 0.001495081\n",
      "Training Loop 0 Epoch: 7100  cost= 0.002358275\n",
      "Training Loop 0 Epoch: 7200  cost= 0.002400185\n",
      "Training Loop 0 Epoch: 7300  cost= 0.002415760\n",
      "Training Loop 0 Epoch: 7400  cost= 0.003791369\n",
      "Training Loop 0 Epoch: 7500  cost= 0.005302627\n",
      "Training Loop 0 Epoch: 7600  cost= 0.003984267\n",
      "Training Loop 0 Epoch: 7700  cost= 0.003536860\n",
      "Training Loop 0 Epoch: 7800  cost= 0.002330382\n",
      "Training Loop 0 Epoch: 7900  cost= 0.001123571\n",
      "Training Loop 0 Epoch: 8000  cost= 0.001074600\n",
      "Training Loop 0 Epoch: 8100  cost= 0.002680547\n",
      "Training Loop 0 Epoch: 8200  cost= 0.003092627\n",
      "Training Loop 0 Epoch: 8300  cost= 0.003234695\n",
      "Training Loop 0 Epoch: 8400  cost= 0.001857303\n",
      "Training Loop 0 Epoch: 8500  cost= 0.001695299\n",
      "Training Loop 0 Epoch: 8600  cost= 0.002180477\n",
      "Training Loop 0 Epoch: 8700  cost= 0.003125192\n",
      "Training Loop 0 Epoch: 8800  cost= 0.002050266\n",
      "Training Loop 0 Epoch: 8900  cost= 0.001552811\n",
      "Training Loop 0 Epoch: 9000  cost= 0.001473934\n",
      "Training Loop 0 Epoch: 9100  cost= 0.003398597\n",
      "Training Loop 0 Epoch: 9200  cost= 0.004546488\n",
      "Training Loop 0 Epoch: 9300  cost= 0.002103746\n",
      "Training Loop 0 Epoch: 9400  cost= 0.002165340\n",
      "Training Loop 0 Epoch: 9500  cost= 0.001824254\n",
      "Training Loop 0 Epoch: 9600  cost= 0.002882814\n",
      "Training Loop 0 Epoch: 9700  cost= 0.004614978\n",
      "Training Loop 0 Epoch: 9800  cost= 0.004597804\n",
      "Training Loop 0 Epoch: 9900  cost= 0.003696377\n",
      "Training Loop 0 Epoch: 10000  cost= 0.003416514\n",
      "save model done.\n",
      "Training Loop 0 Epoch: 10100  cost= 0.005594392\n",
      "Training Loop 0 Epoch: 10200  cost= 0.003729373\n",
      "Training Loop 0 Epoch: 10300  cost= 0.003036524\n",
      "Training Loop 0 Epoch: 10400  cost= 0.001204226\n",
      "Training Loop 0 Epoch: 10500  cost= 0.000813007\n",
      "Training Loop 0 Epoch: 10600  cost= 0.000834625\n",
      "Training Loop 0 Epoch: 10700  cost= 0.000718396\n",
      "Training Loop 0 Epoch: 10800  cost= 0.000752051\n",
      "Training Loop 0 Epoch: 10900  cost= 0.001477033\n",
      "Training Loop 0 Epoch: 11000  cost= 0.002956504\n",
      "Training Loop 0 Epoch: 11100  cost= 0.003195186\n",
      "Training Loop 0 Epoch: 11200  cost= 0.003193022\n",
      "Training Loop 0 Epoch: 11300  cost= 0.003224997\n",
      "Training Loop 0 Epoch: 11400  cost= 0.002438910\n",
      "Training Loop 0 Epoch: 11500  cost= 0.002370915\n",
      "Training Loop 0 Epoch: 11600  cost= 0.002635241\n",
      "Training Loop 0 Epoch: 11700  cost= 0.002205059\n",
      "Training Loop 0 Epoch: 11800  cost= 0.001715441\n",
      "Training Loop 0 Epoch: 11900  cost= 0.001814305\n",
      "Training Loop 0 Epoch: 12000  cost= 0.002132281\n",
      "Training Loop 0 Epoch: 12100  cost= 0.002142836\n",
      "Training Loop 0 Epoch: 12200  cost= 0.001629289\n",
      "Training Loop 0 Epoch: 12300  cost= 0.001845133\n",
      "Training Loop 0 Epoch: 12400  cost= 0.002125962\n",
      "Training Loop 0 Epoch: 12500  cost= 0.003493917\n",
      "Training Loop 0 Epoch: 12600  cost= 0.003783405\n",
      "Training Loop 0 Epoch: 12700  cost= 0.002916540\n",
      "Training Loop 0 Epoch: 12800  cost= 0.002501646\n",
      "Training Loop 0 Epoch: 12900  cost= 0.004942547\n",
      "Training Loop 0 Epoch: 13000  cost= 0.002667778\n",
      "Training Loop 0 Epoch: 13100  cost= 0.001111264\n",
      "Training Loop 0 Epoch: 13200  cost= 0.000758108\n",
      "Training Loop 0 Epoch: 13300  cost= 0.000896875\n",
      "Training Loop 0 Epoch: 13400  cost= 0.002096081\n",
      "Training Loop 0 Epoch: 13500  cost= 0.001995469\n",
      "Training Loop 0 Epoch: 13600  cost= 0.002190194\n",
      "Training Loop 0 Epoch: 13700  cost= 0.003533105\n",
      "Training Loop 0 Epoch: 13800  cost= 0.004752195\n",
      "Training Loop 0 Epoch: 13900  cost= 0.003493325\n",
      "Training Loop 0 Epoch: 14000  cost= 0.002509237\n",
      "Training Loop 0 Epoch: 14100  cost= 0.001731940\n",
      "Training Loop 0 Epoch: 14200  cost= 0.001412431\n",
      "Training Loop 0 Epoch: 14300  cost= 0.002033981\n",
      "Training Loop 0 Epoch: 14400  cost= 0.002739662\n",
      "Training Loop 0 Epoch: 14500  cost= 0.002264801\n",
      "Training Loop 0 Epoch: 14600  cost= 0.001864111\n",
      "Training Loop 0 Epoch: 14700  cost= 0.001847872\n",
      "Training Loop 0 Epoch: 14800  cost= 0.001675398\n",
      "Training Loop 0 Epoch: 14900  cost= 0.001936069\n",
      "Training Loop 0 Epoch: 15000  cost= 0.002378487\n",
      "save model done.\n",
      "Training Loop 0 Epoch: 15100  cost= 0.001628147\n",
      "Training Loop 0 Epoch: 15200  cost= 0.001448540\n",
      "Training Loop 0 Epoch: 15300  cost= 0.001581161\n",
      "Training Loop 0 Epoch: 15400  cost= 0.001812480\n",
      "Training Loop 0 Epoch: 15500  cost= 0.002783106\n",
      "Training Loop 0 Epoch: 15600  cost= 0.005727602\n",
      "Training Loop 0 Epoch: 15700  cost= 0.008138143\n",
      "Training Loop 0 Epoch: 15800  cost= 0.004145654\n",
      "Training Loop 0 Epoch: 15900  cost= 0.001995894\n",
      "Training Loop 0 Epoch: 16000  cost= 0.002384245\n",
      "Training Loop 0 Epoch: 16100  cost= 0.002604576\n",
      "Training Loop 0 Epoch: 16200  cost= 0.002281778\n",
      "Training Loop 0 Epoch: 16300  cost= 0.001802150\n",
      "Training Loop 0 Epoch: 16400  cost= 0.002474083\n",
      "Training Loop 0 Epoch: 16500  cost= 0.003034358\n",
      "Training Loop 0 Epoch: 16600  cost= 0.003122950\n",
      "Training Loop 0 Epoch: 16700  cost= 0.003117303\n",
      "Training Loop 0 Epoch: 16800  cost= 0.003452825\n",
      "Training Loop 0 Epoch: 16900  cost= 0.002797003\n",
      "Training Loop 0 Epoch: 17000  cost= 0.002389826\n",
      "Training Loop 0 Epoch: 17100  cost= 0.003271993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loop 0 Epoch: 17200  cost= 0.002848237\n",
      "Training Loop 0 Epoch: 17300  cost= 0.002157818\n",
      "Training Loop 0 Epoch: 17400  cost= 0.002357084\n",
      "Training Loop 0 Epoch: 17500  cost= 0.002449370\n",
      "Training Loop 0 Epoch: 17600  cost= 0.002721618\n",
      "Training Loop 0 Epoch: 17700  cost= 0.002279713\n",
      "Training Loop 0 Epoch: 17800  cost= 0.001479817\n",
      "Training Loop 0 Epoch: 17900  cost= 0.002085682\n",
      "Training Loop 0 Epoch: 18000  cost= 0.001711413\n",
      "Training Loop 0 Epoch: 18100  cost= 0.001745965\n",
      "Training Loop 0 Epoch: 18200  cost= 0.002646277\n",
      "Training Loop 0 Epoch: 18300  cost= 0.002167450\n",
      "Training Loop 0 Epoch: 18400  cost= 0.002740120\n",
      "Training Loop 0 Epoch: 18500  cost= 0.002571063\n",
      "Training Loop 0 Epoch: 18600  cost= 0.002935673\n",
      "Training Loop 0 Epoch: 18700  cost= 0.003119225\n",
      "Training Loop 0 Epoch: 18800  cost= 0.003052456\n",
      "Training Loop 0 Epoch: 18900  cost= 0.002541199\n",
      "Training Loop 0 Epoch: 19000  cost= 0.002218225\n",
      "Training Loop 0 Epoch: 19100  cost= 0.001531305\n",
      "Training Loop 0 Epoch: 19200  cost= 0.001175721\n",
      "Training Loop 0 Epoch: 19300  cost= 0.001384764\n",
      "Training Loop 0 Epoch: 19400  cost= 0.001327897\n",
      "Training Loop 0 Epoch: 19500  cost= 0.001944586\n",
      "Training Loop 0 Epoch: 19600  cost= 0.003171373\n",
      "Training Loop 0 Epoch: 19700  cost= 0.002353865\n",
      "Training Loop 0 Epoch: 19800  cost= 0.002773827\n",
      "Training Loop 0 Epoch: 19900  cost= 0.003425834\n",
      "Training Loop 0 Epoch: 20000  cost= 0.002377556\n",
      "save model done.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "5_flod_CV step accuracy: [0.01161927]\n",
      "Training Loop 1 Epoch: 0001  cost= 2.267678270\n",
      "Training Loop 1 Epoch: 0100  cost= 0.000893854\n",
      "Training Loop 1 Epoch: 0200  cost= 0.001312917\n",
      "Training Loop 1 Epoch: 0300  cost= 0.000467766\n",
      "Training Loop 1 Epoch: 0400  cost= 0.022341161\n",
      "Training Loop 1 Epoch: 0500  cost= 0.000214649\n",
      "Training Loop 1 Epoch: 0600  cost= 0.000495417\n",
      "Training Loop 1 Epoch: 0700  cost= 0.000758135\n",
      "Training Loop 1 Epoch: 0800  cost= 0.002203203\n",
      "Training Loop 1 Epoch: 0900  cost= 0.002150933\n",
      "Training Loop 1 Epoch: 1000  cost= 0.006427852\n",
      "Training Loop 1 Epoch: 1100  cost= 0.001412601\n",
      "Training Loop 1 Epoch: 1200  cost= 0.003200432\n",
      "Training Loop 1 Epoch: 1300  cost= 0.003389885\n",
      "Training Loop 1 Epoch: 1400  cost= 0.001860461\n",
      "Training Loop 1 Epoch: 1500  cost= 0.004562547\n",
      "Training Loop 1 Epoch: 1600  cost= 0.001406231\n",
      "Training Loop 1 Epoch: 1700  cost= 0.003262144\n",
      "Training Loop 1 Epoch: 1800  cost= 0.002111662\n",
      "Training Loop 1 Epoch: 1900  cost= 0.002920869\n",
      "Training Loop 1 Epoch: 2000  cost= 0.001559447\n",
      "Training Loop 1 Epoch: 2100  cost= 0.002765972\n",
      "Training Loop 1 Epoch: 2200  cost= 0.002297545\n",
      "Training Loop 1 Epoch: 2300  cost= 0.002844554\n",
      "Training Loop 1 Epoch: 2400  cost= 0.002104267\n",
      "Training Loop 1 Epoch: 2500  cost= 0.002636999\n",
      "Training Loop 1 Epoch: 2600  cost= 0.001868661\n",
      "Training Loop 1 Epoch: 2700  cost= 0.002280898\n",
      "Training Loop 1 Epoch: 2800  cost= 0.001882407\n",
      "Training Loop 1 Epoch: 2900  cost= 0.002813832\n",
      "Training Loop 1 Epoch: 3000  cost= 0.001702729\n",
      "Training Loop 1 Epoch: 3100  cost= 0.003037013\n",
      "Training Loop 1 Epoch: 3200  cost= 0.001922566\n",
      "Training Loop 1 Epoch: 3300  cost= 0.001986051\n",
      "Training Loop 1 Epoch: 3400  cost= 0.002212801\n",
      "Training Loop 1 Epoch: 3500  cost= 0.002297527\n",
      "Training Loop 1 Epoch: 3600  cost= 0.002374135\n",
      "Training Loop 1 Epoch: 3700  cost= 0.002133450\n",
      "Training Loop 1 Epoch: 3800  cost= 0.002896278\n",
      "Training Loop 1 Epoch: 3900  cost= 0.001921679\n",
      "Training Loop 1 Epoch: 4000  cost= 0.001767643\n",
      "Training Loop 1 Epoch: 4100  cost= 0.002417687\n",
      "Training Loop 1 Epoch: 4200  cost= 0.002047311\n",
      "Training Loop 1 Epoch: 4300  cost= 0.002738607\n",
      "Training Loop 1 Epoch: 4400  cost= 0.002287882\n",
      "Training Loop 1 Epoch: 4500  cost= 0.002298382\n",
      "Training Loop 1 Epoch: 4600  cost= 0.001809867\n",
      "Training Loop 1 Epoch: 4700  cost= 0.001416639\n",
      "Training Loop 1 Epoch: 4800  cost= 0.001907490\n",
      "Training Loop 1 Epoch: 4900  cost= 0.002666810\n",
      "Training Loop 1 Epoch: 5000  cost= 0.001923648\n",
      "save model done.\n",
      "Training Loop 1 Epoch: 5100  cost= 0.001694686\n",
      "Training Loop 1 Epoch: 5200  cost= 0.001848026\n",
      "Training Loop 1 Epoch: 5300  cost= 0.002691430\n",
      "Training Loop 1 Epoch: 5400  cost= 0.002126465\n",
      "Training Loop 1 Epoch: 5500  cost= 0.001821188\n",
      "Training Loop 1 Epoch: 5600  cost= 0.002256717\n",
      "Training Loop 1 Epoch: 5700  cost= 0.002369808\n",
      "Training Loop 1 Epoch: 5800  cost= 0.001731602\n",
      "Training Loop 1 Epoch: 5900  cost= 0.002042872\n",
      "Training Loop 1 Epoch: 6000  cost= 0.002203650\n",
      "Training Loop 1 Epoch: 6100  cost= 0.002308566\n",
      "Training Loop 1 Epoch: 6200  cost= 0.002224628\n",
      "Training Loop 1 Epoch: 6300  cost= 0.003004738\n",
      "Training Loop 1 Epoch: 6400  cost= 0.002237257\n",
      "Training Loop 1 Epoch: 6500  cost= 0.002140433\n",
      "Training Loop 1 Epoch: 6600  cost= 0.001827412\n",
      "Training Loop 1 Epoch: 6700  cost= 0.001624182\n",
      "Training Loop 1 Epoch: 6800  cost= 0.001882675\n",
      "Training Loop 1 Epoch: 6900  cost= 0.002181516\n",
      "Training Loop 1 Epoch: 7000  cost= 0.001817629\n",
      "Training Loop 1 Epoch: 7100  cost= 0.002059346\n",
      "Training Loop 1 Epoch: 7200  cost= 0.002293405\n",
      "Training Loop 1 Epoch: 7300  cost= 0.001302429\n",
      "Training Loop 1 Epoch: 7400  cost= 0.001834986\n",
      "Training Loop 1 Epoch: 7500  cost= 0.001774153\n",
      "Training Loop 1 Epoch: 7600  cost= 0.001744273\n",
      "Training Loop 1 Epoch: 7700  cost= 0.001977836\n",
      "Training Loop 1 Epoch: 7800  cost= 0.001384133\n",
      "Training Loop 1 Epoch: 7900  cost= 0.001599626\n",
      "Training Loop 1 Epoch: 8000  cost= 0.001213929\n",
      "Training Loop 1 Epoch: 8100  cost= 0.000967722\n",
      "Training Loop 1 Epoch: 8200  cost= 0.001840375\n",
      "Training Loop 1 Epoch: 8300  cost= 0.001992179\n",
      "Training Loop 1 Epoch: 8400  cost= 0.001841166\n",
      "Training Loop 1 Epoch: 8500  cost= 0.001727962\n",
      "Training Loop 1 Epoch: 8600  cost= 0.002572513\n",
      "Training Loop 1 Epoch: 8700  cost= 0.004207649\n",
      "Training Loop 1 Epoch: 8800  cost= 0.001925530\n",
      "Training Loop 1 Epoch: 8900  cost= 0.001248226\n",
      "Training Loop 1 Epoch: 9000  cost= 0.001734642\n",
      "Training Loop 1 Epoch: 9100  cost= 0.001791796\n",
      "Training Loop 1 Epoch: 9200  cost= 0.001895428\n",
      "Training Loop 1 Epoch: 9300  cost= 0.001165768\n",
      "Training Loop 1 Epoch: 9400  cost= 0.001360844\n",
      "Training Loop 1 Epoch: 9500  cost= 0.002251686\n",
      "Training Loop 1 Epoch: 9600  cost= 0.001689271\n",
      "Training Loop 1 Epoch: 9700  cost= 0.002046894\n",
      "Training Loop 1 Epoch: 9800  cost= 0.001285586\n",
      "Training Loop 1 Epoch: 9900  cost= 0.001184097\n",
      "Training Loop 1 Epoch: 10000  cost= 0.001357247\n",
      "save model done.\n",
      "Training Loop 1 Epoch: 10100  cost= 0.001508366\n",
      "Training Loop 1 Epoch: 10200  cost= 0.002628411\n",
      "Training Loop 1 Epoch: 10300  cost= 0.002375620\n",
      "Training Loop 1 Epoch: 10400  cost= 0.002020409\n",
      "Training Loop 1 Epoch: 10500  cost= 0.002600529\n",
      "Training Loop 1 Epoch: 10600  cost= 0.002556535\n",
      "Training Loop 1 Epoch: 10700  cost= 0.001314052\n",
      "Training Loop 1 Epoch: 10800  cost= 0.001424649\n",
      "Training Loop 1 Epoch: 10900  cost= 0.001048349\n",
      "Training Loop 1 Epoch: 11000  cost= 0.001353409\n",
      "Training Loop 1 Epoch: 11100  cost= 0.001675709\n",
      "Training Loop 1 Epoch: 11200  cost= 0.002723776\n",
      "Training Loop 1 Epoch: 11300  cost= 0.002924204\n",
      "Training Loop 1 Epoch: 11400  cost= 0.004148534\n",
      "Training Loop 1 Epoch: 11500  cost= 0.002744761\n",
      "Training Loop 1 Epoch: 11600  cost= 0.000939023\n",
      "Training Loop 1 Epoch: 11700  cost= 0.000837187\n",
      "Training Loop 1 Epoch: 11800  cost= 0.000884522\n",
      "Training Loop 1 Epoch: 11900  cost= 0.001104407\n",
      "Training Loop 1 Epoch: 12000  cost= 0.001040952\n",
      "Training Loop 1 Epoch: 12100  cost= 0.001474924\n",
      "Training Loop 1 Epoch: 12200  cost= 0.002101785\n",
      "Training Loop 1 Epoch: 12300  cost= 0.002904243\n",
      "Training Loop 1 Epoch: 12400  cost= 0.001964252\n",
      "Training Loop 1 Epoch: 12500  cost= 0.002348175\n",
      "Training Loop 1 Epoch: 12600  cost= 0.002717023\n",
      "Training Loop 1 Epoch: 12700  cost= 0.001954339\n",
      "Training Loop 1 Epoch: 12800  cost= 0.001683774\n",
      "Training Loop 1 Epoch: 12900  cost= 0.002018874\n",
      "Training Loop 1 Epoch: 13000  cost= 0.002379103\n",
      "Training Loop 1 Epoch: 13100  cost= 0.001229546\n",
      "Training Loop 1 Epoch: 13200  cost= 0.001292967\n",
      "Training Loop 1 Epoch: 13300  cost= 0.001784318\n",
      "Training Loop 1 Epoch: 13400  cost= 0.002068594\n",
      "Training Loop 1 Epoch: 13500  cost= 0.002050987\n",
      "Training Loop 1 Epoch: 13600  cost= 0.002817923\n",
      "Training Loop 1 Epoch: 13700  cost= 0.001946496\n",
      "Training Loop 1 Epoch: 13800  cost= 0.001821342\n",
      "Training Loop 1 Epoch: 13900  cost= 0.001823396\n",
      "Training Loop 1 Epoch: 14000  cost= 0.002232012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loop 1 Epoch: 14100  cost= 0.002435571\n",
      "Training Loop 1 Epoch: 14200  cost= 0.001093224\n",
      "Training Loop 1 Epoch: 14300  cost= 0.002067856\n",
      "Training Loop 1 Epoch: 14400  cost= 0.001883319\n",
      "Training Loop 1 Epoch: 14500  cost= 0.002149766\n",
      "Training Loop 1 Epoch: 14600  cost= 0.002170822\n",
      "Training Loop 1 Epoch: 14700  cost= 0.002696654\n",
      "Training Loop 1 Epoch: 14800  cost= 0.002053957\n",
      "Training Loop 1 Epoch: 14900  cost= 0.001511528\n",
      "Training Loop 1 Epoch: 15000  cost= 0.001648417\n",
      "save model done.\n",
      "Training Loop 1 Epoch: 15100  cost= 0.001102393\n",
      "Training Loop 1 Epoch: 15200  cost= 0.001241891\n",
      "Training Loop 1 Epoch: 15300  cost= 0.001109855\n",
      "Training Loop 1 Epoch: 15400  cost= 0.001675562\n",
      "Training Loop 1 Epoch: 15500  cost= 0.002082233\n",
      "Training Loop 1 Epoch: 15600  cost= 0.002011656\n",
      "Training Loop 1 Epoch: 15700  cost= 0.002173467\n",
      "Training Loop 1 Epoch: 15800  cost= 0.002365625\n",
      "Training Loop 1 Epoch: 15900  cost= 0.003301967\n",
      "Training Loop 1 Epoch: 16000  cost= 0.003070128\n",
      "Training Loop 1 Epoch: 16100  cost= 0.002563524\n",
      "Training Loop 1 Epoch: 16200  cost= 0.002099581\n",
      "Training Loop 1 Epoch: 16300  cost= 0.001908005\n",
      "Training Loop 1 Epoch: 16400  cost= 0.001732572\n",
      "Training Loop 1 Epoch: 16500  cost= 0.001059839\n",
      "Training Loop 1 Epoch: 16600  cost= 0.001617904\n",
      "Training Loop 1 Epoch: 16700  cost= 0.001530997\n",
      "Training Loop 1 Epoch: 16800  cost= 0.001470755\n",
      "Training Loop 1 Epoch: 16900  cost= 0.001746506\n",
      "Training Loop 1 Epoch: 17000  cost= 0.002270302\n",
      "Training Loop 1 Epoch: 17100  cost= 0.003127394\n",
      "Training Loop 1 Epoch: 17200  cost= 0.002099355\n",
      "Training Loop 1 Epoch: 17300  cost= 0.001899929\n",
      "Training Loop 1 Epoch: 17400  cost= 0.001463520\n",
      "Training Loop 1 Epoch: 17500  cost= 0.001512618\n",
      "Training Loop 1 Epoch: 17600  cost= 0.001573135\n",
      "Training Loop 1 Epoch: 17700  cost= 0.001096369\n",
      "Training Loop 1 Epoch: 17800  cost= 0.000977352\n",
      "Training Loop 1 Epoch: 17900  cost= 0.001039684\n",
      "Training Loop 1 Epoch: 18000  cost= 0.001342971\n",
      "Training Loop 1 Epoch: 18100  cost= 0.001205454\n",
      "Training Loop 1 Epoch: 18200  cost= 0.001857682\n",
      "Training Loop 1 Epoch: 18300  cost= 0.001582079\n",
      "Training Loop 1 Epoch: 18400  cost= 0.001443864\n",
      "Training Loop 1 Epoch: 18500  cost= 0.001473181\n",
      "Training Loop 1 Epoch: 18600  cost= 0.001903466\n",
      "Training Loop 1 Epoch: 18700  cost= 0.002109322\n",
      "Training Loop 1 Epoch: 18800  cost= 0.001828830\n",
      "Training Loop 1 Epoch: 18900  cost= 0.001415769\n",
      "Training Loop 1 Epoch: 19000  cost= 0.001236321\n",
      "Training Loop 1 Epoch: 19100  cost= 0.001644017\n",
      "Training Loop 1 Epoch: 19200  cost= 0.001086539\n",
      "Training Loop 1 Epoch: 19300  cost= 0.001210509\n",
      "Training Loop 1 Epoch: 19400  cost= 0.001005965\n",
      "Training Loop 1 Epoch: 19500  cost= 0.001164562\n",
      "Training Loop 1 Epoch: 19600  cost= 0.001164932\n",
      "Training Loop 1 Epoch: 19700  cost= 0.001515043\n",
      "Training Loop 1 Epoch: 19800  cost= 0.001851777\n",
      "Training Loop 1 Epoch: 19900  cost= 0.002132748\n",
      "Training Loop 1 Epoch: 20000  cost= 0.001994440\n",
      "save model done.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "5_flod_CV step accuracy: [0.00977304]\n",
      "Training Loop 2 Epoch: 0001  cost= 1.693993017\n",
      "Training Loop 2 Epoch: 0100  cost= 0.000638985\n",
      "Training Loop 2 Epoch: 0200  cost= 0.004383225\n",
      "Training Loop 2 Epoch: 0300  cost= 0.000147836\n",
      "Training Loop 2 Epoch: 0400  cost= 0.003563189\n",
      "Training Loop 2 Epoch: 0500  cost= 0.000092976\n",
      "Training Loop 2 Epoch: 0600  cost= 0.016159746\n",
      "Training Loop 2 Epoch: 0700  cost= 0.000151535\n",
      "Training Loop 2 Epoch: 0800  cost= 0.000184494\n",
      "Training Loop 2 Epoch: 0900  cost= 0.001473648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-75d2c1bc5a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mloopcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mevadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ba6f10f228e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mydata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#training settings\n",
    "training_epochs = 20000\n",
    "batch_size = 50\n",
    "display_step = 100\n",
    "save_step = 5000\n",
    "\n",
    "#network structure\n",
    "D_in, D_out = 39, 1\n",
    "D_h1 = 200\n",
    "D_h2 = 100\n",
    "D_h3 = 50\n",
    "D_h4 = 25\n",
    "#D_h5 = 12\n",
    "\n",
    "#initialize\n",
    "x, y, pred, cost, optimizer, init, saver = initialize()\n",
    "\n",
    "action = 'find'\n",
    "action = '5_fold_CV'\n",
    "\n",
    "if action == 'train':\n",
    "    #load data\n",
    "    xdata = readdata('nnin.txt', D_in)\n",
    "    ydata = readdata('nnout.txt', D_out)\n",
    "    num = len(xdata)\n",
    "\n",
    "    #train\n",
    "    loopcount = 1\n",
    "    train()\n",
    "\n",
    "elif action == 'train_CV':\n",
    "    #load data\n",
    "    xdata = readdata('nnin.txt', D_in)\n",
    "    ydata = readdata('nnout.txt', D_out)\n",
    "    num = len(xdata)\n",
    "    \n",
    "    #clean and shuffle and reassign data\n",
    "    xdata, ydata = clean()\n",
    "    xdata, ydata = shuffledata()\n",
    "    xdata, ydata, edata, rdata = reassign(5)\n",
    "    num = len(xdata)\n",
    "    \n",
    "    loopcount = 0\n",
    "    train_CV(200)\n",
    "    \n",
    "    \n",
    "elif action == 'find':\n",
    "    #finding the 300 pieces of data\n",
    "    find()\n",
    "    \n",
    "elif action == 'evaluate':\n",
    "    edata = readdata('nne.txt', D_in)\n",
    "    evaluate(True)\n",
    "    \n",
    "elif action == '5_fold_CV':\n",
    "    xdatatotal = readdata('nnin.txt', D_in)\n",
    "    ydatatotal = readdata('nnout.txt', D_out)\n",
    "    numtotal = len(xdatatotal)\n",
    "    datatotal = []\n",
    "    for i in range(numtotal):\n",
    "        xdatatotal[i].extend(ydatatotal[i])\n",
    "        datatotal.append(xdatatotal[i])\n",
    "    random.shuffle(datatotal)\n",
    "    \n",
    "    finalscore = 0\n",
    "    for i in range(1):\n",
    "        xdata = []\n",
    "        ydata = []\n",
    "        edata = []\n",
    "        rdata = []\n",
    "        for j in range(numtotal):\n",
    "            if j<=int(numtotal/5*(i+1)) and j>=int(numtotal/5*i):\n",
    "                edata.append([datatotal[j][k] for k in range(D_in)])\n",
    "                rdata.append([datatotal[j][D_in]])\n",
    "            else:\n",
    "                xdata.append([datatotal[j][k] for k in range(D_in)])\n",
    "                ydata.append([datatotal[j][D_in]])\n",
    "        num = len(xdata)\n",
    "        \n",
    "        loopcount = i\n",
    "        train()\n",
    "        evadata = evaluate(False)\n",
    "        score = 0\n",
    "        for j in range(len(evadata)):\n",
    "            score = score + abs(evadata[j]-rdata[j])/len(evadata)\n",
    "        print('5_flod_CV step accuracy:', score)\n",
    "        finalscore = finalscore + score/5\n",
    "    print('5_flod_CV average accuracy:', finalscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdatatotal = np.asarray(readdata('nnin.txt', D_in))\n",
    "ydatatotal = np.asarray(readdata('nnout.txt', D_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1281, 39)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdatatotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1281, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydatatotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
